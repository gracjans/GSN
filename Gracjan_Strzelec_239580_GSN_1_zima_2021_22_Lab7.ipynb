{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4URUV8_K6JG"
      },
      "source": [
        "# GSN 1 - Winter 2021/22 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9v3ojf_SvyV",
        "outputId": "bb78bcdf-5e5c-45b2-8e97-597719959422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17k8g_EJSvX3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import trange\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow import keras\n",
        "from scipy import signal\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f98UJ3Fah2xZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "myNetwork class parameters:\n",
        "      neurons_count: int (number of neurons in layer)     \n",
        "      inputs_count: int (number of inputs to each neuron of layer)\n",
        "      weights: OPTIONAL - tuple of arrays (arrays of weights for each of layer)\n",
        "      biases: OPTIONAL - tuple of arrays (arrays of biases for each of layer)\n",
        "\"\"\"\n",
        "\n",
        "class myLayer:\n",
        "    def __init__(self, inputs_count, neuron_count, learning_rate=0.01, weights=None, biases=None):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # check if weights or biases are passed as an network argument, otherwise initialize it by default\n",
        "        if weights is not None:\n",
        "            self.weights = np.array(weights)  \n",
        "        else:  \n",
        "            self.weights = np.random.normal(scale=0.01, size=(neuron_count, inputs_count))\n",
        "\n",
        "        if biases is not None:\n",
        "            self.biases = np.array(biases)\n",
        "        else:  \n",
        "            self.biases = np.zeros(neuron_count)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return np.dot(inputs, self.weights.T) + self.biases\n",
        "\n",
        "    def backward(self, input, grad):\n",
        "        grad_input = np.dot(grad, self.weights)\n",
        "        grad_weights = np.dot(input.T, grad)\n",
        "        grad_biases = grad.mean(axis=0) * input.shape[0]\n",
        "\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights.T\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases.T\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYg_W0AVbDjx"
      },
      "outputs": [],
      "source": [
        "class Conv2D():\n",
        "  def __init__(self, input_shape, n_filters, filter_shape, padding='none', stride = 1, learning_rate=0.01):\n",
        "      self.learning_rate = learning_rate\n",
        "      self.inputShape = input_shape\n",
        "      if len(input_shape) == 4:\n",
        "        self.batches, self.inputHeight, self.inputWidth, self.inputDepth = input_shape\n",
        "      elif len(input_shape) == 5:\n",
        "        self.batches, self.prevFilters, self.inputHeight, self.inputWidth, self.inputDepth = input_shape\n",
        "\n",
        "      self.padding = 0\n",
        "      self.stride = stride\n",
        "      self.numFilters = n_filters \n",
        "      self.filterShape = filter_shape \n",
        "      self.filtersShape = (self.numFilters, self.filterShape[0], self.filterShape[1], self.inputDepth) \n",
        "\n",
        "      self.outShape = (self.batches, self.numFilters, self.inputHeight - self.filterShape[0] +1, self.inputWidth - self.filterShape[1] + 1, self.inputDepth)\n",
        "      self.kernels = np.random.randn(*self.filtersShape)\n",
        "      self.biases = np.random.randn(*self.outShape)\n",
        "\n",
        "  def forward(self, input):\n",
        "      self.input = input\n",
        "      if len(input.shape) == 4:\n",
        "        batches, h, w, depth = input.shape\n",
        "        self.output = np.copy(self.biases)\n",
        "        for batch in range(batches):\n",
        "            for filter in range(self.numFilters):\n",
        "              for depth in range(self.inputDepth):\n",
        "                self.output[batch, filter, :, :, depth] += signal.correlate2d(self.input[batch,:,:,depth], self.kernels[filter,:,:,depth], \"valid\")\n",
        "      elif len(input.shape) == 5:\n",
        "        batches, prevFilters, h, w, depth = input.shape\n",
        "        self.output = np.copy(self.biases)\n",
        "        for batch in range(self.batches):\n",
        "          for prevFilter in range(prevFilters):\n",
        "            for filter in range(self.numFilters):\n",
        "              for depth in range(self.inputDepth):\n",
        "                self.output[batch, filter, :, :, depth] += signal.correlate2d(self.input[batch,prevFilter,:,:,depth], self.kernels[filter,:,:,depth], \"valid\") \n",
        "      return self.output\n",
        "\n",
        "  def backward(self, input, grad):\n",
        "      self.input = input\n",
        "      if len(input.shape) == 4:\n",
        "        batches, h, w, depth = input.shape\n",
        "        _, prevFilters, _, _, _ = grad.shape\n",
        "        kernels_gradient = np.zeros(self.filtersShape)\n",
        "        input_gradient = np.zeros(input.shape)\n",
        "        for batch in range(batches):\n",
        "          for prevFilter in range(prevFilters):\n",
        "            for filter in range(self.numFilters):\n",
        "              for depth in range(self.inputDepth):\n",
        "                kernels_gradient[filter,:,:,depth] = signal.correlate2d(self.input[batch, :, :, depth], grad[batch,prevFilter,:,:,depth], \"valid\")\n",
        "                input_gradient[batch, :, :, depth] += signal.convolve2d(grad[batch,prevFilter,:,:,depth], self.kernels[filter,:,:,depth], \"full\")\n",
        "\n",
        "      elif len(input.shape) == 5:\n",
        "        batches, _, h, w, depth = input.shape\n",
        "        _, prevFilters, _, _, _ = grad.shape\n",
        "        kernels_gradient = np.zeros(self.filtersShape)\n",
        "        input_gradient = np.zeros(input.shape)\n",
        "        for batch in range(self.batches):\n",
        "          for prevFilter in range(prevFilters):\n",
        "            for filter in range(self.numFilters):\n",
        "              for depth in range(self.inputDepth):\n",
        "                kernels_gradient[filter,:,:,depth] = signal.correlate2d(self.input[batch, filter, :, :, depth], grad[batch,prevFilter,:,:,depth], \"valid\")\n",
        "                input_gradient[batch, filter, :, :, depth] += signal.convolve2d(grad[batch,prevFilter,:,:,depth], self.kernels[filter,:,:,depth], \"full\")\n",
        "      \n",
        "      self.kernels -= self.learning_rate * kernels_gradient\n",
        "      self.biases -= self.learning_rate * grad\n",
        "      return input_gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQND4wBUv4nJ"
      },
      "outputs": [],
      "source": [
        "class Pooling2D():\n",
        "    def __init__(self, kernel_size, stride, padding=0, pool_mode = 'max' ):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.pool_mode = pool_mode\n",
        "\n",
        "    def forward(self, input):\n",
        "        batches, num_filters, h, w, depth = input.shape\n",
        "        input = np.pad(input, self.padding, mode='constant')\n",
        "        output_shape = (batches, num_filters, (h - self.kernel_size) // self.stride + 1, (w - self.kernel_size) // self.stride + 1, depth)\n",
        "        output = np.zeros(output_shape)\n",
        "\n",
        "        for batch in range(batches):\n",
        "          for filter in range(num_filters):\n",
        "            for depth in range(depth):\n",
        "              shape_w = (output_shape[2], output_shape[3], self.kernel_size, self.kernel_size)\n",
        "              strides_w = (self.stride*input.strides[2], self.stride*input.strides[3], input.strides[2], input.strides[3])\n",
        "              A_w = as_strided(input, shape_w, strides_w)\n",
        "              if self.pool_mode == 'max':\n",
        "                  output[batch, filter, :, :, depth] = A_w.max(axis=(2, 3))\n",
        "\n",
        "              elif self.pool_mode == 'avg':\n",
        "                  output[batch, filter, :, :, depth] = A_w.mean(axis=(2, 3))\n",
        "        return output\n",
        "\n",
        "    def backward(self, input, grad, mode = \"max\"):\n",
        "        batches, num_filters, n_H_prev, n_W_prev, depth = input.shape\n",
        "        _, _, n_H, n_W, n_C = grad.shape\n",
        "    \n",
        "        input_gradient = np.zeros(input.shape)\n",
        "          \n",
        "        for i in range(batches): \n",
        "            for filter in range(num_filters):\n",
        "              for h in range(n_H):                   \n",
        "                for w in range(n_W):               \n",
        "                  for c in range(n_C):           \n",
        "                      vert_start = h*self.stride\n",
        "                      vert_end = vert_start + self.kernel_size\n",
        "                      horiz_start = w*self.stride\n",
        "                      horiz_end = horiz_start + self.kernel_size\n",
        "              \n",
        "                      if mode == \"max\":\n",
        "                          input_slice = input[i, filter, vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "                          mask = (input_slice == np.max(input_slice))\n",
        "                          input_gradient[i, filter, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, grad[i, filter, h, w, c])\n",
        "                              \n",
        "                      elif mode == \"average\":\n",
        "                          da = grad[i, filter, h, w, c]\n",
        "                          shape = (self.kernel_size, self.kernel_size)\n",
        "                          input_gradient[i, filter, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
        "        return input_gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgkw6zjwbX_4"
      },
      "outputs": [],
      "source": [
        "class Flatten():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.prevShape = input.shape\n",
        "        batches, num_filters, h, w, depth = input.shape\n",
        "        return input.reshape(batches, num_filters*h*w*depth)\n",
        "\n",
        "    def backward(self, input, grad):\n",
        "        return grad.reshape(self.prevShape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EptYkxODT-wn"
      },
      "outputs": [],
      "source": [
        "# ACTIVATION FUNCTIONS\n",
        "\n",
        "class sigmoid():\n",
        "    def forward(self, x):\n",
        "      return 1 / (1 + np.exp(-X))\n",
        "    def backward(self, x, grad):\n",
        "      sig_grad = x * (1 - x)\n",
        "      return grad * sig_grad\n",
        "\n",
        "class relu():\n",
        "    def forward(self, x):\n",
        "      return x * (x > 0)\n",
        "    def backward(self, x, grad):\n",
        "      re_grad = 1 * (x > 0)\n",
        "      return grad * re_grad\n",
        "\n",
        "class tanh():\n",
        "    def forward(self, x):\n",
        "      return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "    def backward(self, x, grad):\n",
        "      tan_grad = 1 - (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))**2\n",
        "      return grad * tan_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d06otz2gmo2k"
      },
      "outputs": [],
      "source": [
        "def softmax_crossentropy_with_logits(logits, reference_answers):\n",
        "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
        "    xentropy = -logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits, reference_answers):\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-IatISClrGp"
      },
      "outputs": [],
      "source": [
        "class myNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers_instances = []\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers_instances.append(layer)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = [inputs]\n",
        "\n",
        "        current_state = np.array(inputs)\n",
        "        for layer in self.layers_instances:\n",
        "          current_state = layer.forward(current_state)\n",
        "          self.inputs.append(current_state)\n",
        "        return current_state\n",
        "\n",
        "    def backward(self, grad):\n",
        "        current_state = grad\n",
        "        for layer_index in range(len(self.layers_instances))[::-1]:\n",
        "          layer = self.layers_instances[layer_index]\n",
        "          current_state = layer.backward(self.inputs[layer_index], current_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmt7OhwyzPse"
      },
      "outputs": [],
      "source": [
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs)) \n",
        "\n",
        "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize, desc='minibatch iteration'):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IULKy-C9ZmWx"
      },
      "source": [
        "### Initialize and train neural network with MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1Q59c8rzu1g",
        "outputId": "5b2982c6-87d4-4556-d35e-6ed14c35ca68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 28, 28, 1)\n",
            "(60000, 28, 28, 1)\n",
            "(60000,)\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "#X_train, X_test = np.reshape(X_train / 255., (60000, 28*28)), np.reshape(X_test /255. , (10000, 28 * 28))\n",
        "\n",
        "X_train, X_test = X_train / 255, X_test /255\n",
        "X_train = X_train.reshape(60000, 28, 28, 1)\n",
        "X_test = X_test.reshape(10000, 28, 28, 1)\n",
        "\n",
        "print(X_test.shape)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vPqMxrVzWGV",
        "outputId": "f26aba9d-c25e-4a1c-c73a-e6fdcb0e65fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "minibatch iteration: 100%|██████████| 1875/1875 [04:03<00:00,  7.70it/s]\n",
            "minibatch iteration: 100%|██████████| 10000/10000 [01:01<00:00, 161.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1 \t Loss: 1.0004405326792272 \t Test accuracy: 0.8898 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "minibatch iteration: 100%|██████████| 1875/1875 [04:02<00:00,  7.72it/s]\n",
            "minibatch iteration: 100%|██████████| 10000/10000 [01:01<00:00, 162.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2 \t Loss: 0.32332958831091213 \t Test accuracy: 0.924 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "minibatch iteration: 100%|██████████| 1875/1875 [04:03<00:00,  7.71it/s]\n",
            "minibatch iteration: 100%|██████████| 10000/10000 [01:03<00:00, 157.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3 \t Loss: 0.2321569134462044 \t Test accuracy: 0.9429 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "minibatch iteration: 100%|██████████| 1875/1875 [04:02<00:00,  7.74it/s]\n",
            "minibatch iteration: 100%|██████████| 10000/10000 [01:04<00:00, 155.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4 \t Loss: 0.1759306098119565 \t Test accuracy: 0.9559 \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "minibatch iteration: 100%|██████████| 1875/1875 [04:04<00:00,  7.66it/s]\n",
            "minibatch iteration: 100%|██████████| 10000/10000 [01:02<00:00, 160.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 5 \t Loss: 0.1405370052916032 \t Test accuracy: 0.9542 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "### full network training ###\n",
        "net = myNetwork()\n",
        "\n",
        "net.add_layer(Conv2D((32, 28, 28, 1), 4, (3,3)))\n",
        "net.add_layer(relu())\n",
        "net.add_layer(Flatten())\n",
        "net.add_layer(myLayer(2704, 512))\n",
        "net.add_layer(relu())\n",
        "net.add_layer(myLayer(512, 64))\n",
        "net.add_layer(relu())\n",
        "net.add_layer(myLayer(64, 10))\n",
        "\n",
        "for epoch in range(5):\n",
        "    epoch_loss = []\n",
        "    for x_batch, y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
        " \n",
        "        ### forward ###\n",
        "        logits = net.forward(x_batch)\n",
        "\n",
        "        ### loss calculation ###\n",
        "        loss = softmax_crossentropy_with_logits(logits, y_batch)\n",
        "        epoch_loss.append(np.mean(loss))\n",
        "\n",
        "        ### backward ###\n",
        "        loss_grad = grad_softmax_crossentropy_with_logits(logits, y_batch)\n",
        "\n",
        "        net.backward(loss_grad)\n",
        "\n",
        "    preds = []\n",
        "    for x_batch, _ in iterate_minibatches(X_test, y_test, batchsize=1, shuffle=False):\n",
        "        preds.append(np.argmax(net.forward(x_batch)))\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1} \\t Loss: {np.mean(epoch_loss)} \\t Test accuracy: {accuracy_score(y_test, preds)} \\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Gracjan_Strzelec_239580_GSN_1_zima_2021_22_Lab7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
